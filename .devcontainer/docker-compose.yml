version: '3.8'

# =============================================================================
# Docker Compose for Kodiak SecOps 1 Training
# =============================================================================
# Usage:
#   docker compose -f .devcontainer/docker-compose.yml up -d
#   docker compose -f .devcontainer/docker-compose.yml exec training bash
# =============================================================================

services:
  training:
    build:
      context: ..
      dockerfile: .devcontainer/Dockerfile
    container_name: kodiak-secops-training

    # GPU Configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use "1" for single GPU, "all" for multi-GPU
              capabilities: [gpu]

    # Shared memory for PyTorch DataLoader
    shm_size: '16gb'

    # Remove memory limits for training
    ulimits:
      memlock: -1
      stack: 67108864

    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - HF_TOKEN=${HF_TOKEN:-}
      - WANDB_API_KEY=${WANDB_API_KEY:-}
      - WANDB_PROJECT=kodiak-secops-1
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-all}
      # Memory optimization
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

    # Volume mounts
    volumes:
      # Project directory
      - ..:/workspace:cached
      # Persistent cache for models (avoid re-downloading)
      - huggingface_cache:/workspace/.cache/huggingface
      - torch_cache:/workspace/.cache/torch
      # Training outputs
      - training_outputs:/workspace/outputs
      # Host HF cache (optional - uncomment if you have models locally)
      # - ~/.cache/huggingface:/workspace/.cache/huggingface:cached

    working_dir: /workspace

    # Keep container running
    stdin_open: true
    tty: true

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD", "nvidia-smi"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: TensorBoard service
  tensorboard:
    image: tensorflow/tensorflow:latest
    container_name: kodiak-secops-tensorboard
    ports:
      - "6006:6006"
    volumes:
      - training_outputs:/logs:ro
    command: tensorboard --logdir=/logs --host=0.0.0.0 --port=6006
    depends_on:
      - training
    profiles:
      - monitoring

  # Optional: Jupyter Lab service
  jupyter:
    build:
      context: ..
      dockerfile: .devcontainer/Dockerfile
    container_name: kodiak-secops-jupyter
    ports:
      - "8888:8888"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ..:/workspace:cached
      - huggingface_cache:/workspace/.cache/huggingface
    environment:
      - JUPYTER_TOKEN=kodiaksecops
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
    profiles:
      - notebook

# Persistent volumes for caching
volumes:
  huggingface_cache:
    driver: local
  torch_cache:
    driver: local
  training_outputs:
    driver: local
